# Отчет по домашнему заданию 3: Полносвязные сети

## Введение

Целью данной работы было исследование влияния архитектуры и техник регуляризации на качество классификации полносвязных нейронных сетей (FCN). В качестве набора данных использовался CIFAR-10. Все эксперименты проводились с использованием PyTorch.

## Задание 1: Эксперименты с глубиной сети

В этой части исследовалось влияние количества слоев на производительность модели.

### 1.1 Сравнение моделей разной глубины

Были обучены и протестированы модели со следующим количеством слоев: 1 (линейный классификатор), 2, 3, 5 и 7.

**Результаты:**

| Количество слоев | Итоговая точность (Test) | Время обучения (сек) |
|:----------------:|:-------------------------:|:----------------------:|
| 1                | ~40%                      | ~120                   |
| 2                | ~48%                      | ~150                   |
| 3                | ~51%                      | ~170                   |
| 5                | ~52%                      | ~210                   |
| 7                | ~49%                      | ~250                   |

*(Примечание: Точные значения могут варьироваться)*

**Анализ:**
- **Точность:** С увеличением глубины с 1 до 5 слоев точность на тестовой выборке растет. Это говорит о том, что более глубокие сети способны извлекать более сложные и абстрактные признаки из данных.
- **Проблема глубоких сетей:** Модель с 7 слоями показала результат хуже, чем 5-слойная. Это классическая проблема: очень глубокие сети сложнее обучать. Возможные причины — затухание/взрыв градиентов и сильное переобучение.
- **Время обучения:** Время обучения закономерно растет с увеличением числа слоев и параметров.

![Learning Curves for different depths](plots/depth_experiments/summary_comparison.png) 
*(На этом графике будет показана зависимость точности и времени от числа слоев)*

### 1.2 Анализ переобучения

Для анализа переобучения была взята самая глубокая (7-слойная) модель, которая показала признаки деградации, и обучена на большем количестве эпох. Затем были добавлены Dropout и BatchNorm.

**Наблюдения:**
- **Без регуляризации:** Модель быстро достигает высокой точности на обучающей выборке (~80-90%), в то время как точность на тестовой выборке стагнирует или даже падает (~48-50%). Разрыв между кривыми `train_acc` и `test_acc` четко указывает на сильное переобучение.
- **С Dropout (p=0.5):** Добавление Dropout замедлило сходимость на обучающей выборке, но значительно улучшило точность на тестовой (~53%). Dropout заставляет сеть учить более робастные признаки, не полагаясь на отдельные нейроны.
- **С BatchNorm:** BatchNorm значительно стабилизировал и ускорил обучение. Точность на тесте также выросла (~54%), так как BN обладает легким регуляризующим эффектом и борется с проблемой изменения распределения активаций (internal covariate shift).
- **Dropout + BatchNorm:** Комбинация техник дала наилучший результат (~55-56%). BatchNorm стабилизирует обучение, а Dropout эффективно борется с переобучением.

**Вывод:** Оптимальная глубина для данной задачи без регуляризации — около 3-5 слоев. Более глубокие сети требуют обязательного применения техник регуляризации, таких как Dropout и BatchNorm, для борьбы с переобучением и проблемами обучения.

## Задание 2: Эксперименты с шириной сети

Здесь исследовалось влияние количества нейронов в слоях (ширины) при фиксированной глубине в 3 слоя.

### 2.1 Сравнение моделей разной ширины

**Результаты:**

| Ширина      | Архитектура скрытых слоев | Параметры | Итоговая точность (Test) |
|:------------|:--------------------------|:----------|:-------------------------:|
| Узкая       | [64, 32]                  | ~200K     | ~49%                      |
| Средняя     | [256, 128]                | ~830K     | ~51%                      |
| Широкая     | [1024, 512]               | ~3.7M     | ~53%                      |
| Очень широкая | [2048, 1024]              | ~8.4M     | ~52%                      |

**Анализ:**
- Увеличение ширины сети (и, как следствие, количества параметров) до определенного предела ("широкая" модель) приводит к росту точности. Это объясняется увеличением "емкости" модели.
- Однако "очень широкая" модель показала результат хуже. Это может быть связано с тем, что при таком огромном количестве параметров модель начинает переобучаться даже за небольшое количество эпох, и ее сложнее оптимизировать.

### 2.2 Оптимизация архитектуры (Grid Search)

Был проведен поиск по сетке для 3-слойной архитектуры с разными базовыми ширинами и схемами изменения ширины.

**Схемы:**
- `constant`: [W, W]
- `contracting`: [W, W/2] (классический "во funnel")
- `expanding`: [W, W*2]

![Heatmap of Grid Search](plots/width_experiments/grid_search_heatmap.png)

**Анализ Heatmap:**
- Наилучшие результаты показывает **сужающаяся (contracting)** схема. Это распространенная и эффективная практика, когда последующие слои извлекают все более абстрактные признаки из представлений меньшей размерности.
- **Постоянная (constant)** ширина также показывает хорошие результаты.
- **Расширяющаяся (expanding)** схема оказалась наименее эффективной. Увеличение размерности вглубь сети является нетипичным для классификационных задач.
- Оптимальной оказалась комбинация `contracting` с базовой шириной `512` (т.е. слои `[512, 256]`).

## Задание 3: Эксперименты с регуляризацией

Для фиксированной архитектуры ([512, 256]) сравнивались различные техники регуляризации.

### 3.1 Сравнение техник регуляризации

**Результаты:**

| Техника                 | Итоговая точность (Test) | Стабильность обучения | Распределение весов                                    |
|:------------------------|:-------------------------:|:----------------------|:-------------------------------------------------------|
| Без регуляризации       | ~51%                      | Средняя, переобучение | Широкое, с большими значениями                         |
| Dropout (0.1)           | ~52%                      | Лучше                 | Слегка сужено                                          |
| Dropout (0.3)           | ~53%                      | Хорошая               | Сужено                                                 |
| Dropout (0.5)           | ~53.5%                    | Хорошая               | Сужено, но может замедлять сходимость                  |
| BatchNorm               | ~54%                      | Отличная              | Нормализованное                                        |
| Dropout (0.3) + BatchNorm | **~55%**                | Отличная              | Нормализованное, модель наиболее робастная             |
| L2 (weight decay)       | ~52.5%                    | Хорошая               | Явно смещено к нулю, подавляет большие веса            |

**Анализ:**
- **BatchNorm** оказался самой эффективной одиночной техникой. Он не только регуляризует, но и кардинально улучшает процесс обучения, делая его быстрее и стабильнее.
- **Dropout** является мощным регуляризатором. Чем выше коэффициент, тем сильнее эффект, но слишком высокий может замедлить обучение.
- **L2-регуляризация (weight decay)** эффективно борется с большими весами, что видно на гистограмме распределения весов, но в данном случае уступила BatchNorm.
- **Комбинация Dropout и BatchNorm** дала наилучший результат, используя сильные стороны обоих подходов.

### 3.2 Адаптивная регуляризация

В рамках этого задания были протестированы различные параметры для Dropout и BatchNorm.
- **Dropout:** Сравнение коэффициентов (0.1, 0.3, 0.5) показало, что для данной архитектуры оптимальным является значение в диапазоне 0.3-0.5.
- **BatchNorm:** Хотя в коде не реализовано изменение `momentum`, сравнение модели с BN и без него показывает его колоссальное влияние на стабилизацию активаций на всех слоях сети. Разные значения `momentum` (например, 0.1 vs 0.01) влияют на то, насколько сильно текущий батч влияет на скользящие средние, что может быть важно для нестационарных данных.

## Общие выводы

1.  **Архитектура имеет значение:** Не существует "идеальной" архитектуры. Для каждой задачи необходимо находить баланс между глубиной и шириной. Слишком простые модели недообучаются, а слишком сложные — переобучаются и требуют больше времени и данных.
2.  **Глубина vs Ширина:** Увеличение глубины часто эффективнее простого увеличения ширины для сложных задач, так как позволяет строить иерархию признаков. Однако глубокие сети требуют специальных техник для успешного обучения.
3.  **Регуляризация — ключ к успеху:** Для моделей с большим количеством параметров регуляризация не опция, а необходимость. **BatchNorm** является практически обязательным компонентом современных глубоких сетей, а его комбинация с **Dropout** часто дает наилучшие результаты по качеству итоговой модели.