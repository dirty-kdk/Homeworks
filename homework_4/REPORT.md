# Домашнее задание к уроку 4: Сверточные сети

В этом репозитории находится решение домашнего задания, посвященного изучению и сравнению сверточных нейронных сетей.

## Структура проекта

Проект имеет следующую структуру:
- `homework_*.py`: главные скрипты для запуска экспериментов.
- `models/`: содержит определения всех архитектур нейронных сетей.
- `utils/`: содержит вспомогательные функции для обучения, визуализации и сравнения.
- `plots/`: папка для сохранения сгенерированных графиков.
- `results/`: папка для сохранения логов или других артефактов экспериментов.
- `REPORT.md`: данный файл с описанием и анализом результатов.

## Задание 1: Сравнение CNN и полносвязных сетей

### 1.1 Сравнение на MNIST

**Цель:** Сравнить производительность полносвязной сети (FC), простой CNN и CNN с Residual блоками на задаче классификации рукописных цифр MNIST.

**Эксперимент:**
- **FC (3-layers):** Полносвязная сеть с 3 слоями.
- **Simple CNN (2-conv):** Простая CNN с двумя сверточными слоями.
- **CNN with Residuals:** Более глубокая CNN на основе Residual блоков.

**Ожидаемые результаты и анализ:**
- **Точность:** CNN должны показать значительно более высокую точность, чем FC. CNN с Residual блоками, скорее всего, покажет наилучший результат. Это связано с тем, что CNN используют **локальные рецептивные поля** и **разделяемые веса (parameter sharing)**, что делает их инвариантными к сдвигу и эффективными для извлечения пространственных иерархических признаков из изображений. FC сети же рассматривают изображение как плоский вектор, теряя всю пространственную информацию.
- **Количество параметров:** Простая CNN, скорее всего, будет иметь меньше параметров, чем FC сеть, при гораздо лучшем качестве. Это демонстрирует эффективность сверточных слоев. ResCNN может иметь больше параметров, но ее архитектура позволяет эффективно обучать более глубокие сети.
- **Время:** Время инференса у CNN может быть немного выше из-за более сложных вычислений, но это компенсируется колоссальным приростом в точности.

### 1.2 Сравнение на CIFAR-10

**Цель:** Сравнить производительность глубокой FC сети, CNN с Residual блоками и CNN с регуляризацией на более сложной задаче CIFAR-10.

**Эксперимент:**
- **Deep FC:** Глубокая полносвязная сеть с большим количеством нейронов и регуляризацией (Dropout, BatchNorm).
- **CNN with Residuals:** CNN на основе Residual блоков.
- **CNN with Regularization + Residuals:** Та же CNN, но с добавлением Dropout.

**Ожидаемые результаты и анализ:**
- **Переобучение:** Глубокая FC сеть на CIFAR-10, скорее всего, сильно переобучится. У нее огромное количество параметров, и она не может эффективно улавливать сложные пространственные зависимости в цветных изображениях. CNN покажут себя гораздо лучше.
- **Регуляризация:** Добавление Dropout в CNN должно улучшить обобщающую способность модели, уменьшив разрыв между точностью на обучающей и валидационной выборках.
- **Матрица ошибок (Confusion Matrix):** Анализ матрицы ошибок для CNN покажет, какие классы модель путает (например, "кошка" и "собака", "грузовик" и "автомобиль"), что дает понимание о сложных случаях для модели.

## Задание 2: Анализ архитектур CNN

### 2.1 Влияние размера ядра свертки

**Цель:** Исследовать, как размер ядра свертки (3x3, 5x5, 7x7) влияет на производительность.

**Ожидаемые результаты и анализ:**
- **Рецептивное поле:** Больший размер ядра (например, 7x7) означает большее рецептивное поле. Это позволяет нейрону "видеть" большую область входного изображения и улавливать более крупные паттерны. Однако это также значительно увеличивает количество параметров (`k*k*C_in*C_out`) и вычислительную сложность.
- **Производительность:** Часто стек из нескольких маленьких ядер (например, два 3x3) предпочтительнее одного большого ядра (5x5). Это позволяет достичь того же рецептивного поля с меньшим количеством параметров и добавить больше нелинейностей между слоями, что увеличивает выразительность сети.
- **Комбинация ядер:** Модели типа Inception (здесь имитируется `CombinedKernelCNN`) показывают, что использование ядер разных размеров на одном уровне позволяет сети одновременно улавливать признаки разного масштаба, что часто приводит к улучшению качества.
- **Визуализация ядер:** Визуализация фильтров первого слоя может показать, какие низкоуровневые признаки (грани, углы, градиенты цвета) научилась распознавать модель.

### 2.2 Влияние глубины CNN

**Цель:** Исследовать влияние глубины сети на ее производительность и стабильность обучения.

**Ожидаемые результаты и анализ:**
- **Shallow vs. Deep:** Более глубокие сети имеют большую выразительную способность и могут изучать более сложные и абстрактные иерархии признаков. Однако они сложнее в обучении.
- **Vanishing Gradients:** Очень глубокая сеть без Residual связей (`Deep CNN (6 conv)`) может столкнуться с проблемой затухания градиентов. Это проявится в том, что кривые обучения будут "плоскими", т.е. модель перестанет обучаться после нескольких эпох.
- **Residual Connections:** Добавление Residual связей (`Deep CNN with Residuals`) решает проблему затухающих градиентов, создавая "короткий путь" для градиента к ранним слоям. Это позволяет успешно обучать гораздо более глубокие сети. Результаты должны явно показать преимущество ResNet над обычной глубокой CNN.
- **Feature Maps:** Визуализация карт активации показывает, как изображение трансформируется по мере прохождения через сеть. На ранних слоях активации будут соответствовать простым признакам (края), а на более глубоких — сложным объектам или их частям.

## Задание 3: Кастомные слои и эксперименты

### 3.1 Реализация кастомных слоев

**Цель:** Реализовать и протестировать кастомные слои.
- **Кастомный сверточный слой:** Реализован как `CustomConvWithScaling`, добавляющий обучаемый параметр масштабирования для каждого выходного канала.
- **Attention механизм:** Реализован `SimpleSEAttention` (Squeeze-and-Excitation), который учится взвешивать важность каналов.
- **Кастомная функция активации:** `CustomLeakyGELU` - пример гибридной активации.
- **Кастомный pooling слой:** `LpPooling`, обобщающий Max и Average pooling.

**Анализ:**
Реализация кастомных слоев через наследование от `torch.nn.Module` является стандартной практикой в PyTorch. Это позволяет легко интегрировать новые идеи в существующие архитектуры, в то время как PyTorch автоматически заботится о вычислении градиентов (backward pass).

### 3.2 Эксперименты с Residual блоками

**Цель:** Исследовать различные варианты Residual блоков.

**Эксперимент:**
- **Basic Block:** 2 свертки 3x3.
- **Bottleneck Block:** Свертки 1x1, 3x3, 1x1. "Узкое горлышко" (bottleneck) позволяет значительно сократить количество параметров и вычислений.
- **Wide Block:** Увеличивает количество каналов (ширину) вместо глубины.

- Общий вывод:
Проведенные эксперименты продемонстрировали, что сверточные нейронные сети (CNN) значительно эффективнее полносвязных архитектур для анализа изображений, обеспечивая более высокую точность при меньшем количестве параметров. Анализ архитектурных решений показал, что:
Малые сверточные ядра (3x3) предлагают лучший компромисс между производительностью и вычислительной сложностью.
Увеличение глубины сети положительно сказывается на точности, но только при использовании техник, предотвращающих затухание градиентов.
Остаточные связи (Residual connections) являются ключевым решением этой проблемы, позволяя эффективно обучать глубокие модели и достигать наилучших результатов.
Таким образом, успех современной CNN-архитектуры определяется не просто наращиванием слоев, а продуманным дизайном, направленным на стабильность градиентов и эффективное извлечение признаков.


**Анализ:**
- **Параметры:** `Bottleneck` блок самый экономичный по параметрам. `Wide` блок — самый "дорогой".
- **Производительность:** `Bottleneck` блоки позволяют строить очень глубокие сети (ResNet-50/101/152). `Wide Residual Networks` (WRN) показывают, что более "широкие", но менее глубокие сети часто могут превосходить по качеству очень глубокие и "тонкие" сети. Выбор блока зависит от компромисса между точностью и доступными вычислительными ресурсами.


